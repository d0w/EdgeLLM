# Example configuration for distributed LLM inference
networks:
  - name: "llama-7b-network"
    model: "meta-llama/Llama-2-7b-hf"
    total_layers: 32
    nodes:
      - id: "gpu-node-1"
        layers: [0, 1, 2, 3, 4, 5, 6, 7]
        gpu_memory: "8GB"
        location: "datacenter-1"
        
      - id: "gpu-node-2" 
        layers: [8, 9, 10, 11, 12, 13, 14, 15]
        gpu_memory: "8GB"
        location: "datacenter-2"
        
      - id: "gpu-node-3"
        layers: [16, 17, 18, 19, 20, 21, 22, 23]
        gpu_memory: "8GB"  
        location: "edge-device-1"
        
      - id: "gpu-node-4"
        layers: [24, 25, 26, 27, 28, 29, 30, 31]
        gpu_memory: "8GB"
        location: "edge-device-2"

  - name: "gpt-xl-network"
    model: "gpt2-xl"
    total_layers: 48
    nodes:
      - id: "powerful-node-1"
        layers: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]
        gpu_memory: "16GB"
        
      - id: "powerful-node-2"
        layers: [16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]
        gpu_memory: "16GB"
        
      - id: "powerful-node-3"
        layers: [32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47]
        gpu_memory: "16GB" 